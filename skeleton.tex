% UG project example file, February 2022
%   A minior change in citation, September 2023 [HS]
% Do not change the first two lines of code, except you may delete "logo," if causing problems.
% Understand any problems and seek approval before assuming it's ok to remove ugcheck.
\documentclass[logo,bsc,singlespacing,parskip]{infthesis}
\usepackage{ugcheck}
\usepackage{xcolor}  % Package for colors
\usepackage{listings}
\usepackage{comment}
\usepackage{amsmath}
\usepackage[most]{tcolorbox}
\usepackage{tabularx} % Include this in the preamble
\usepackage{float}
\usepackage{longtable}
\usepackage{hyperref}


\usepackage{amssymb}
\input{commands.tex}

\lstdefinelanguage{Koka}{
  keywords={fun, var, match, val, if, then, else, do,  type, struct},
  morekeywords=[2]{return,resume, handle, ctl, with,effect},
  morekeywords=[3]{Sstate, Ready, Blocked, Proc, Nil, Cons, string, int},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}

\lstset{
  language=Koka,
  basicstyle=\scriptsize\ttfamily,
  keywordstyle=\color{purple},          % Group 1: basic syntax
  keywordstyle=[2]\color{blue},         % Group 2: effect-related
  keywordstyle=[3]\color{teal},         % Group 3: data constructors
  commentstyle=\color{green!80!black},
  stringstyle=\color{orange},
  breaklines=true,
  columns=fullflexible,
  showstringspaces=false
}

\tcbset{
  examplebox/.style={
    colback=blue!5!white,
    colframe=blue!75!black,
    boxrule=0.8pt,
    arc=2mm,
    top=1mm,
    bottom=1mm,
    left=1mm,
    right=1mm,
    fonttitle=\bfseries,
    title={Example},
    sharp corners,
    breakable,
    enhanced jigsaw
  }
}



% Include any packages you need below, but don't include any that change the page
% layout or style of the dissertation. By including the ugcheck package above,
% you should catch most accidental changes of page layout though.

\usepackage{microtype} % recommended, but you can remove if it causes problems
\usepackage{cite} % recommended for citations

\begin{document}
\begin{preliminary}

\title{Implementing Unix using Effect Handlers}

\author{Douglas Torrance}

% CHOOSE YOUR DEGREE a):
% please leave just one of the following un-commented
%\course{Artificial Intelligence}
%\course{Artificial Intelligence and Computer Science}
%\course{Artificial Intelligence and Mathematics}
%\course{Artificial Intelligence and Software Engineering}
%\course{Cognitive Science}
\course{Computer Science}
%\course{Computer Science and Management Science}
%\course{Computer Science and Mathematics}
%\course{Computer Science and Physics}
%\course{Software Engineering}
%\course{Master of Informatics} % MInf students

% CHOOSE YOUR DEGREE b):
% please leave just one of the following un-commented
%\project{MInf Project (Part 1) Report}  % 4th year MInf students
%\project{MInf Project (Part 2) Report}  % 5th year MInf students
\project{4th Year Project Report}        % all other UG4 students


\date{\today}

\abstract{

Most imperative programming languages do not provide a structured facility for handling effects, and functional approaches such as Monads lead to poor modularity and composability issues \cite{monad}. 


Recently, though, Algebraic effect handlers have emerged as a flexible, first-class mechanism to define and interpret effects dynamically. This approach provides a structured and modular way to reason about computational effects, such as exceptions, state, and concurrency, and is particularly useful in modelling complex control flows. As a result, it proves effective in addressing some of the challenges involved in implementing a Unix-like system.
 \cite{}. 


Unix was first developed in the 1970s at Bell Labs \cite{Unix} to provide a new approach to operating system design. Prior to this, operating systems were typically monolithic and hardware specific. Unix was designed to be a small, portable and multi-user operating system, which emphasised simplicity, modularity and composability. Its key innovations were: 

\begin{itemize}
    \item Everything is treated as a file – a unified approach that simplifies interaction with devices, processes, and data. 
    \item Pipes and redirection - enable seamless composition of commands into larger programs.
    \item Simplified interface - offers a minimal set of system calls, ensuring a consistent approach to process management, file handling, and I/O.
\end{itemize}

This dissertation consists of two parts. In the first part I implement the "Tiny Unix" system described in Hillerström’s \textit{Foundations for Programming and Implementing Effect Handlers} \cite{hillerstrom_foundations_nodate}. Written in the Koka programming language, this implementation provides a modular and composable approach to Unix functionality. 

In the second part, I apply algebraic equivalence rules from Pretnar’s \textit{An Introduction to Algebraic Effects and Handlers} \cite{pretnar_introduction_2015} to analyse the implementation. By leveraging these rules, we can evaluate its properties and compare it to standard Unix implementations. Since certain algebraic standard effects have associated equational theories, we are also able to verify that our implementation adheres to these theoretical foundations.


}

\maketitle

\newenvironment{ethics}
   {\begin{frontenv}{Research Ethics Approval}{\LARGE}}
   {\end{frontenv}\newpage}

\begin{ethics}
This project was planned in accordance with the Informatics Research
Ethics policy. It did not involve any aspects that required approval
from the Informatics Research Ethics committee.

\standarddeclaration
\end{ethics}


\begin{acknowledgements}
I would like to express my sincere thanks to Dr. Sam Lindley for his helpful suggestions, insightful feedback, and valuable input throughout the development of this dissertation. His guidance has been greatly appreciated. I would also like to thank Jesse Sigal for his support and for generously sharing his knowledge and time, which were both immensely helpful during my research.
\end{acknowledgements}


\tableofcontents
\end{preliminary}


\chapter{Introduction}
\section{Motivation}
Effect handlers are increasingly being explored as a way to manage computational effects in programming. Unix provides a useful model for exploring their application, as its abstract system calls serve as an interface for programs to interact with the OS, similarly to how effects provide an interface for programs to cause side effects. 

In this project, we implement these system calls using effect handlers, representing each as an algebraic effect to achieve a concise and modular design. Effect handlers offer a simple and principled way to model Unix’s complex state and control flow challenges, while also enabling formal reasoning about our solutions. I will demonstrate how effect handlers support composable implementations and enhance our ability to reason about intricate control flows, one of their key advantages.
Key motivations and observations include:
\begin{itemize}
    \item \textcolor{red}{Unix’s abstract system calls resemble the concept of effect handlers, in that both represent abstract effectful operations whose implementations can vary depending on context.}
    \item \textcolor{red}{Modelling Unix introduces complex control and scoping issues, which can be handled more naturally in effect-oriented languages compared to imperative approaches. }
\end{itemize}


\section{Aims}
This dissertation aims to:
\begin{itemize}
    \item Outline the theory behind effect handler-oriented programming and provide relevant background on the Unix system, explaining the suitability in using effect handlers to model Unix.
    \item Implement the "Tiny Unix" system in Koka as described in Hillerström's thesis \cite{hillerstrom_foundations_nodate}.
    \item Apply algebraic reasoning techniques to verify that our implementation meets the required Unix specifications. 
    \item Reflect on the effectiveness of using these formal techniques.
    
\end{itemize}

\section{Outline}

\textbf{Chapter 2} introduces the background and research surrounding effect handlers. It provides a brief overview of the Unix system and explains why it serves as an interesting model to implement using effect handlers. The chapter also explores how effects give rise to computational trees, enabling modular reasoning about code.

\textbf{Chapter 3} outlines the implementation of the "Tiny Unix" system, as described abstractly in Hillerström's PhD thesis, using the Koka language. It discusses the advantages of modelling certain control flow and scoping issues with effect handlers, as well as the challenges encountered during implementation.

\textbf{Chapter 4} focuses on reasoning about the “Tiny Unix” implementation. It examines attempts to verify the correctness of features such as environment variables, the file system, nondeterminism, and process synchronisation using algebraic reasoning techniques.

\textbf{Chapter 5} presents the conclusions drawn from this project. It reflects on the difficulties of reasoning formally about effectful programs, the lessons learned, and suggests areas where these reasoning techniques might be more effectively applied in future work.


\chapter{Background}

\section{Algebraic Effects}

Algebraic effects \cite{plotkin_handling_2013} and their handlers \cite{pretnar_introduction_2015} provide a structured approach to managing computational side effects, explicitly denoting effectful computations and handling them non-locally.

We describe an effect by first defining its effect signature, which specifies the operations it can perform along with their input parameters and return types. This signature serves as the interface through which side effects are executed. We then abstract the concept of an effect signature, deferring the implementation details of handling these effects to a later stage. Any function that uses this effect (unless it is responsible for handling it itself) must include the effect in its function signature. 

Effects must ultimately be handled by an effect handler, allowing them to be handled differently based on the scope in which they were raised. When the runtime encounters an effect it searches through successively outer scopes until it finds the appropriate effect handler. This allows the effect to be handled in different ways depending on the scope in which the effect was raised. The effect handler will eventually run the code which carries out the intended side effect. 

Effect handler’s separate the core business logic of the program from the implementation details of handling side effects. This separation prevents side-effect handling logic from being scattered throughout the codebase, thereby improving modularity. 

For example, consider logging data accesses in a database. Typically, logging logic would need to be implemented in every data access function, violating the Single Responsibility Principle, which states a class (or module) should have only one reason to change. However, by using an effect handler, you can define a logging effect and centralize all logging logic in one place. This modular approach allows for easy composition of effectful operations. 

In languages like Java, chaining effectful operations can obscure where effects occur and how errors are handled, making it difficult to reason about the interactions between operations. Effect handlers address this issue, making them particularly useful for implementing side effects such as asynchronous operations, state management, logging, and exception handling. 

In the context of functional programming, effect handlers provide a simple and lightweight method for managing side effects. They serve as a viable alternative to monads, offering a more flexible approach. Unlike monads, which can become rigid once defined, effect handlers allow for greater modification. Additionally, they simplify the combination of multiple side effects, avoiding the complexity of monad transformers. In some cases, effect handlers also offer better performance by eliminating the overhead of repeatedly wrapping and unwrapping values within monadic structures. 


\section{Syntax for Effect Handlers}

\subsection{Effect System}
Effect Handler oriented languages have what is known as an effect system \cite{bauer_effect_2013}. This describes the computational effects that may occur when a piece of code is executed. An effect system is typically an extension of a type system. Effect systems can be used to enforce effect safety, ensuring all effects are handled and functions only perform the side effects denotd in their effect signature



\lstset{
backgroundcolor=\color{gray!5}, % Set background color
    basicstyle=\ttfamily,              % Use monospace font
    frame=none,                      % Add a frame around the pseudocode
    numbers=none,                      % No line numbers
    tabsize=4                          % Set tab size
}




\subsection{Effect Type}
An effect type provides an explicit way to denote the side effects that a function performs. Each effect type can have multiple effectful operations. Each effectful operation can have parameters with value types and a valued return type. Note that operations can also produce effects and therefore have effect types.

\begin{lstlisting}
effect exception
    ctl exn(error_msg : string) : a 

fun safe_div (x : int, y: int) : exn int
    if y == 0 then exn("div by zero") else return x/y
\end{lstlisting}

Pure functions use the unit effect type. This shows it has no side effects. 

\begin{lstlisting}
fun add (x: Int, y: int) : () int {
	return x + y
}
\end{lstlisting}

\section{Shallow vs Deep Handlers}

Deep handlers \cite{hillerstrom_foundations_nodate} handlers can handle all the effects caused by a computation. When the continuation is captured in a handler, the captured continuation is also wrapped in the handler. This means that deep handlers can handle effects, which themselves invoke effects. 

In contrast, shallow handlers \cite{ryu_shallow_2018} only manages the first effect caused by a computation. The resumed program no longer includes the handler and the programmer needs to provide a new handler for an effect the resumption may perform.
Deep and Shallow handlers can simulate each others behaviour. However it may make more sense to use one type over another in certain contexts. Most languages only support one or the other. As of 2024 deep handlers are the more popular choice amongst effect oriented languages.

\subsection{Effects as Computational Trees}

We can think of effects forming a computational tree, we can illustrate the difference between Shallow and Deep handlers by describing how each evaluates this computational tree differently. The leaves represent finished computations i.e. pure return values. Each effect is a node which passes control to an effect handler for interpretation. From the operation, the effect handler receives what is known as a resumption, a first class syntactic representation of the continuation, i.e. the remaining computation immediately following the operation being handled. 

The effect handler can; resume the computation with a value (think of a read operation returning its result), cause branching in the computational tree by introducing multiple resumptions, or it could just return a value skipping resuming the computation entirely (as is the case in an exception ). 

A deep handler for an effect is like a fold over the computational tree which provides an interpretation to each instance of that effect given its leaves and subtrees. Just like a fold the deep handler will recursively handle the current operation, and all of its subtrees (continuations). This makes deep handlers easy to reason about as we can assume the way that the continuation will be interpreted will be consistent and has an algebraic structure.

However a shallow handler does not recurse like this, it is only applied to the first effect it sees. It must be explicitly reapplied to continuations if we want their effects to be interpreted too. This makes it harder to do formal reasoning for but it makes it more flexible control over how you handle effects.

\section{Reasoning}


\section{Unix}
Unix is an operating system developed by Bell Labs in the 1970s. Its aim is to create a portable, multi-user, multi-tasking system. It is responsible for managing the:
\begin{itemize}
    \item Processes: creating, managing process communication, terminating processes, forking processes
    \item Scheduling
    \item Basic IO
    \item User and user environment management
\end{itemize}

Unix is built on the “Unix Philosophy” which follows the principles of simplicity and modularity. The “everything is a file” philosophy of Unix provides a consistent interface for us to interact with system resources. This will allow us to separate functionality into modules and implement them using effect handlers.

\section{Evaluating Previous work}
Daniel Hillerström has implemented a theoretical implementation of Unix in his 2021 paper “Foundations for Programming and Implementing Effect Handlers” \cite{hillerstrom_foundations_nodate}. He makes analogy between operating systems and effect handlers that both interpret a series of abstract commands, in the case of the OS this is system calls, in the case of effect handlers, this is operations. The composition of effect handlers, which he views as “tiny operating systems” can provide semantics for a Unix implementation. He uses deep handlers to implement; multiple user sessions, time-sharing and file IO. This paper will use Koka to create a concrete implementation of the abstract syntax he used in his paper. Then we will implement proof using algebraic equivalences to show our implementation holds.

There are several papers which have examined reasoning about effect handlers. Pretnar's "The Logic and Handling of Algebraic Effects" \cite{Pretnar:2010} uses algebraic equations to reason about abstract properties of state, exceptions and nondeterminism. The PhD thesis "Relational Reasoning for Effects and Handlers" by McLaughlin \cite{McLaughlin2020} used automated reasoning techniques to reason about reason about the correctness of an effect handler implementation for Unix-style pipes. 

Perhaps the most interesting is "Not by equations alone: Reasoning with extensible effects" by Oleg Kiselyov, Shin-Cheng Mu and Amr Sabry \cite{kiselyov2021extensible}. This paper outlines the limitations of using equational laws to reason about effectful programs. It shows how composition can break down algebraic laws which held in isolation. It suggests stronger notions of congruence, i.e. proving expressions are equivalent under different contexts are required.



\chapter{Unix Implementation}
Having described the theoretical benefits of effect handlers in the previous chapter, we now show the practical implementation of the "Tiny Unix" system, written by Hillerström in the Koka programming language. 

The features we implement serve as good examples of problems that are difficult to model in traditional programming paradigms. These include problems concerning dynamic scoping, global state, control flow interruptions, and reasoning about suspended computations.

We will show how Koka allows us to implement each effect independently and then compose them together to express this behaviour cleanly and concisely.
In his thesis, Hillerström uses a pseudo-lambda calculus that can switch between deep and shallow handlers. Koka primarily uses deep handlers, enabling us to implement each effect independently, expressing complex behaviour in a clean and concise manner.



\section{Process Status}
In Unix, processes provide an exit code upon termination. A return code of 0 signifies successful execution, while any non-zero value indicates an error. 

We begin by defining the exit effect, which includes a single exit operation parameterised by an integer:
\begin{lstlisting}
effect exit
  ctl exit(n : int) : a
\end{lstlisting}

\vspace{1em}
We can now write the exit handler. It can be seen that the function's signature expects the parameter action as a paused computation, which produces an exit effect.

\begin{lstlisting}
fun status(action : () -> <exit|e> a) : e int
    with final ctl exit(code) code
        action()
        0
\end{lstlisting}
In Hillerström’s language, it was necessary to introduce an absurd function to coerce the valueless return into a form that made sense within the language’s type system. In contrast, Koka allows us to return a value directly without requiring a resumption. In cases where a function returns without explicitly calling exit(n), we assume it has exited successfully and simply return 0.

\section{Basic IO}
Hillerström first models a simplified form of state. From the effect signature, we can see that writes can be labelled with a file descriptor. However, in our implementation, we defer this functionality until later. Initially, we treat all writes as if they are directed to the same output stream, which we conceptualise as stdout, regardless of the specified file descriptor.

\begin{lstlisting}
effect bio
    fun writeBio( fd : filedesc, s : string ) : ()
\end{lstlisting}
\vspace{1em}
Here we define the handler:

\begin{lstlisting}
fun bio( action : () -> <bio|e> a ) : e (a,string)
  var buf := ""    
  with handler
    return (x) -> (x, buf)
    fun writeBio(fd, s) -> { buf := buf ++ s }
  action()
\end{lstlisting}

Note that, unlike in Hillerström’s implementation, we do not require explicit \lstinline{resume} keywords. In Koka, effects of type \lstinline{fun} perform the effective operation and then continue execution immediately.

\section{Users and Environment Variables}
Environment variables are key-value pairs stored in the environment of a process. In this implementation, provide an example by storing the current user's name as an environment variable.

We define users as a sum type:
\begin{lstlisting}
type user = Root, Alice, Bob
\end{lstlisting}

We define two effects: \lstinline{su}, which is parameterised by the user to switch to, and \lstinline{whoami()}, which returns the current user as a string.

\begin{lstlisting}
effect su
  ctl su( u : user ) : ()

effect whoami
  fun whoami() : string
\end{lstlisting}


\begin{lstlisting}
fun env( user : user, action : () -> <whoami|e> a ) : e a
  with fun whoami() 
    match user
      Root  -> "root"
      Alice -> "alice"
      Bob   -> "bob"
  action()
\end{lstlisting}

Next, we define separate handlers for the \lstinline{su} and \lstinline{whoami()} effects:
\begin{lstlisting}
fun session-mgr( initial-user : user, action : () -> <su,whoami|e> a ) : e a
  with env(initial-user)
  with ctl su( u : user )
         mask<whoami>
           with env(u)
           resume(())
  action()

fun env( user : user, action : () -> <whoami|e> a ) : e a
  with fun whoami() 
    match user
      Root  -> "root"
      Alice -> "alice"
      Bob   -> "bob"
  action()
\end{lstlisting}

The \lstinline{env} handler is parameterised by the current user, enabling it to handle \lstinline{whoami()} operations by returning this user's identity.

\lstinline{session_mgr} manages user switches by rewrapping the continuation in a new \lstinline{env} handler, parameterised by the new user. This method offers greater modularity compared to global variables because it fully encapsulates state within the effect handler itself. This is an example of dynamic scoping, a technique that will be especially useful in the next section, which deals with process forking.

\section{Multi-Process System}
\subsection*{Forking}
Hillerstrom describes how forking is a nondeterministic operation as from the perspective of the process making the Fork call. It is unclear in which order the processes will execute as this is managed by the operating system's scheduling policy. 

\begin{lstlisting}
effect fork
  ctl fork() : bool     

fun forking( action : () -> <fork|e> a ) : e list<a>
  with handler
    return(x) [x]
    ctl fork() resume(True) ++ resume(False)
  action()
\end{lstlisting}

We can see that the forking handler introduces two resumptions, splitting computation down two paths. The resumption with True represents the parent and the resumption with False represents the child. We return these resumptions as a list which can be used by the scheduler.

This is designed to be used in code such as:
\begin{lstlisting}
    if fork() {
        // ... parent specific code
    }
    else{
        /// ... child specific code
    }
\end{lstlisting}
This is a simple way of writing code for two programs within the same file.

\subsection*{Scheduling}
Fork has given us the capability to spawn new processes, it returns these as list of paused processes. We must implement a scheduling algorithm to interleave the running of these two processes.

First we must be able to differentiate between a process that has finished and one which is just temporarily paused. We introduce a data type to represent these two process states called \texttt{pstate}. 
\begin{lstlisting}
type pstate<e,a>
  Done(result : a)
  Paused(resumption : () -> e pstate<e,a> )
\end{lstlisting}

Currently there is no way for a process to indicate it is ready to pause, once it is started it will execute until it has finished. We introduce a way of yielding control to the scheduler to allow the interleaving of process execution.

We introduce an effect called `interrupt` and an effect handler called `reify process` which allows us to capture the continuation of a process after it has called interrupt().
\begin{lstlisting}
effect interrupt
  ctl interrupt() : ()

fun reify-process(action : () -> <interrupt|e> a) : e pstate<e,a> 
  with handler  
    return(x) -> Done(x)
    ctl interrupt() -> Paused(fn() resume(()))
  action()
\end{lstlisting}
In our simplified system this must be done manually by the programmer. We could introduce wrapped functions such as \texttt{interrupt-write} however the programmer must be aware that due to non-determinism, these writes may not occur consecutively in the file:
\begin{lstlisting}
interrupt-write("abc");
interrupt-write("def");
\end{lstlisting}


Finally we must introduce a scheduler to coordinate the running of our processes: 
\begin{lstlisting}
fun scheduler( pstates : list<pstate<<fork,div|e>,a>> ) : <div|e> list<a>
  fun schedule( todos : list<pstate<<fork,div|e>,a>>, done : list<a> ) : <div|e> list<a>
    match todos
      Nil -> done
      Cons(Done(x),ps') -> schedule(ps', Cons(x,done))
      Cons(Paused(m),ps') ->
        val ps = forking( m )
        schedule( ps' ++ ps, done )
  schedule(pstates,[])

fun timeshare( action : () -> <fork,interrupt,div|e> a ) : <div|e> list<a>
  val p = Paused( fn() reify-process(action) )
  scheduler([p])
\end{lstlisting}
It maintains two lists of processes, one containing paused processes and one containing finished processes. It works recursively; it runs the first process in the paused until it reaches an interrupt, then it runs the scheduler again with tehe process added to the end of the paused queue. If a process is finished it is added to the done queue. This procedure repeats until there are no processes left in the paused queue.

\section{Serial File System}
Here we implement a more realistic version of the file system we implemented earlier. A unix file system is implemented on top of an array of bytes. Unix structures this medium into several key regions to facilitate file management:
\begin{itemize}
    \item Directory Region: This stores mappings between human-readable filenames and their corresponding Inode numbers.
    \item Inode Region: The Inode list contains meta data about each file
    \item Data Region: This stores the actual contents of file which are accessed via pointers stored in the iNode
\end{itemize}

We define our file system as a record type, composed of the these three lists and two integers to count where the next inode and data regions should be allocated.

\begin{lstlisting}
type directory 
  Directory
    d_list : list<(string, int)>

type dataRegion 
  DataRegion
    dr : list<(int, string)>

type iNode
  INode
    no : int
    loc : int

type iList 
  IList
    il : list<(int, iNode)> 

struct fileSystem
  dir   : directory
  ilist : iList
  dreg  : dataRegion
  dnext : int
  inext : int

\end{lstlisting}

We define the initial file system state \texttt{fs0} as:
\begin{lstlisting}
val fs0 : fileSystem =
  FileSystem (
    Directory([("stdout", 0)]),
    IList([(0, INode ( 1, loc = 0 ))]),
    DataRegion([(0, "")]),
    1,
    1
  )
\end{lstlisting}

\subsection{File Reading and Writing}

We define effects for reading and writing for our file system. The effect \texttt{read} is parameterised with an iNode number and returns an \texttt{Option<String>} type which indicates success or failure to read from the specified iNode. The \texttt{write} operation takes an integer to denote the iNode and a string to append to the file associated with this iNode. It does not indicate whether or not the write succeeded.

\begin{lstlisting}
effect fileRW
    fun read( ino : int ) : option<string>
    fun write( ino : int, s : string ) : ()
\end{lstlisting}



\begin{lstlisting}
fun fread( ino : int, fs : fileSystem ) : <div,fail|e> string
    val inode = lookupINode(ino, fs.ilist)
    val ret = lookupDataRegion(inode.loc, fs.dreg)
    ret
\end{lstlisting}

The various \texttt{lookup} utility functions must account for the possibility that they could fail. We introduce a \texttt{fail} effect and an associated \texttt{withDefault} handler to 

\begin{lstlisting}
effect fail
  ctl fail() : a

fun withDefault<a>(default: a, m: () -> <div,fail|e>a) : <div|e>a
  with handler
    return(x) -> x
    ctl fail() -> default
  m()
\end{lstlisting}

\begin{lstlisting}
fun fileRW<a>(m: () -> <fileRW,div,state<fileSystem>|e>a) : <state<fileSystem>,div|e>  a {
  with handler
    fun read(ino) {
      val cs = withDefault(None, fn() -> Some(fread(ino, get())))
      cs
    }
    fun write(ino, cs) {
      withDefault((), fn() {
          val fsys  = get()
          val fsys' = fwrite(ino, cs, fsys)
          put(fsys')
          ()
      })
    }
  m()
}
\end{lstlisting}

This is the handler for read and write operations in our system. We wrap \texttt{read} and \texttt{write} operations with \texttt{withDefault} to provide error handling, we make failures in lookup and modify functions correspond to None.

\subsection{File Creation and Opening}

We define effects for creating and opening files:

\begin{lstlisting}
effect fileCO
  ctl create( fname : string ) : option<int>
  ctl open( fname : string ) : option<int>
\end{lstlisting}

In a real unix system when a process opens a file, they receive a file descriptor that refers to an entry in the open file descriptor table. This is adds an entry to a global 'Open File Table'. The OFT tracks; which files are open, how they are being accessed (read, write), at which offsets they are being accessed. We present a simplified version which means we don't provide shared state management in the same way. There are no restrictions to concurrent reading or writing. Our opening file mechanism simply returns the associated iNode from the file name.

\begin{lstlisting}
fun fopen(fname: string, fs : fileSystem) 
    match fs.dir {
        Directory(d_list) ->
        match d_list {
            Nil        -> fail()  
            Cons((k, v), rest) ->
            if k == fname then v
            else lookupDirectory(fname, Directory(rest))
        }
    }
\end{lstlisting}    


When creating a file we must handle two possibilities:
\begin{itemize}
    \item The file already exists: in this case we must remove the file contents and simply return the associated  iNode and the modified File System.
    \item The file doesn't exist: in this case we must first allocate space in the data region, then allocate an iNode and then add the file name to the directory. It returns the iNode of this file and the modified FileSystem as a tuple.
\end{itemize}

\begin{lstlisting}
fun fcreate(fname: string, fs: fileSystem) : <div,fail> (int, fileSystem) 
  if has(fname, fs.dir) then {
    val ino = fopen(fname, fs)  
    val inode = lookupINode(ino, fs.ilist)  
    val dreg' = modifyDataRegion(inode.loc, "", fs.dreg)  

    (ino, FileSystem(fs.dir, fs.ilist, dreg', fs.dnext, fs.inext))  
  }
  else {
    val loc = fs.dnext  // next free data block index
    val dreg' = DataRegion(Cons((loc, ""), fs.dreg.dr))  
    val ino = fs.inext  // next free i-node index
    val inode = INode(loc, 1)  
    val ilist' = IList(Cons((ino, inode) , fs.ilist.il))  
    val dir' = Directory(Cons((fname, ino), fs.dir.d_list))
    
    val fs' = FileSystem(dir', ilist', dreg', fs.dnext + 1, fs.inext + 1)  
    (ino, fs')
  }

fun has<a, b>(key: string, xs: directory) : <div> bool
    withDefault(False, fn() {
        val disc = lookupDirectory(key, xs)  
        True  
    })
\end{lstlisting}

Now we can implement the handler for the Create an Open operations.

\begin{lstlisting}
fun fileCreateOpen<a>(action : () -> <fileCO, div, state<fileSystem>|e> a) : <state<fileSystem>,div|e> a
  with handler {
    return(x) -> x
    ctl create(name) {
      val maybeIno = withDefault(None, fn() {
        val fs0 = get()
        val (ino,fs1) = fcreate(name, fs0)
        put(fs1)
        Some(ino)
      })
      resume(maybeIno)
    }

    ctl open(name) {
      val maybeIno = withDefault(None, fn() {
        val fs0 = get()
        val ino = fopen(name, fs0)
        Some(ino)
      })
      resume(maybeIno)}  
  }
  action()
\end{lstlisting}

\subsection{Stream Redirection}
We define the stream redirection operator \texttt{>} to allow us to redirect output streams from processes to files. This is an essential feature for composing commands, enabling pipelines as it allows us to build pipelines without need for explicit intermediate storage.


\begin{lstlisting}
    //
\end{lstlisting}

\subsection{File Linking and Unlinking}
We will implement two new operations, linking and unlinking files. There are 2 main types of link in Unix, hard and symbolic. We will implement hard links, this means that multiple file names can point to the same iNode. This allows us to create multiple references to the same underlying file.

\begin{lstlisting}
effect fileLU
  ctl link(src: string, tgt : string) :()
  ctl unlink(tgt : string) :()
\end{lstlisting}


\begin{lstlisting}
fun flink(src : string, dest : string, fs : fileSystem) : <div,fail> fileSystem {
  if has(dest, fs.dir) then
    fail()  // If `dest` exists, fail
  else {
    val ino = lookupDirectory(src, fs.dir)

    val dir' = Directory(Cons((dest, ino), fs.dir.d_list))

    val inode = lookupINode(ino, fs.ilist)

    val inode' = INode(inode.no, inode.loc + 1)

    val ilist' = modifyINode(ino, inode', fs.ilist)

    FileSystem(dir', ilist', fs.dreg, fs.dnext, fs.inext)
  }
}
\end{lstlisting}


\section{Piping}
Piping is a core feature of Unix. It is so important as it allows the composability of processes. This allows programs to focus on writing small, modular which they can compose together rather than having to write monolithic programs.

These signatures show the produce and consume effects required to implement piping. \lstinline{produce} is parameterised by a value of type b allowing it to yield a value to the read-end. Conversley the \lstinline{consume} has a return type of \lstinline{b}, indicating that it awaits for a value.
\begin{lstlisting}
effect produce<b>
  ctl produce(x : b) : ()   

effect consume<b>
  ctl consume() : b         

\end{lstlisting}

Using shallow handlers, pipe and copipe mutually recurse to handle \lstinline{consume} and \lstinline{produce} effects. In our system this is handled as a function, whereas in Unix these are implemented as processes which allow for concurrency.

\begin{lstlisting}
fun pipe<a,b,e>(p : () -> <produce<b>|e> a,c : () -> <consume <b>|e> a) : e a
    with raw ctl consume()
        copipe(fn(x) rcontext.resume-shallow(x), p)
        return(x) -> x

fun copipe(c : b -><consume<b>, div|e>, c : () -> <produce<b>, div|e> a): <div|e> a
    with raw ctl produce(y)
        pipe(fn() rcontext.resume-shallow(()), fn() c(y))
\end{lstlisting}

\section{Process Synchronisation}

In this section we introduce more advanced techniques for process scheduling. The current system is purely non-deterministic, it does not give us any control over how these processes interact. This new scheduler allows us to coordinate processes by controlling the order in which processes are executed, vital for executing tasks which have temporal dependencies. This introduces more invariants and partial ordering.

We begin by defining a new set of effects. We update the forking mechanism \texttt{ufork()} to a return an integer instead of a boolean. The operation is still intended to return twice; it returns the process identifier (PID) to the parent and 0 to the child process. The \texttt{interrupt()} operation is exactly the same as before, invoked to suspend the current process and allow another to run. The \texttt{wait} operation is parametrised with an integer, which represents the process which must complete before the invoking process can continue. The invoking process will be blocked until this happens.
\begin{lstlisting}
effect co
  ctl ufork() : int
  ctl wait(pid: int): ()
  ctl sinterrupt() :()
\end{lstlisting}


Then we define data types. \texttt{proc} represents a reified process i.e. a process that has been captured into a function which can run later. It returns a list of type list<int, a> which represents the PID and results of all the processes created during the execution of this process.  \texttt{pstate}, this represents the internal scheduling state of a single process. It is a sum type composed of two constructors, denoting if a process is Ready or Blocked. The Blocked constructor has an additional field, PID, which is the process identifier of the process that we are waiting on. \texttt{sstate}, this represents the complete state of the scheduler, it keeps track of everything it needs to manage and schedule multiple processes. It stores; a list of processes to execute, a list of completed processes, the currently executing process and pnext which is used to provide new process ids.
\begin{lstlisting}
rec type proc<a>
  Proc
    p: sstate<a> -> list<(int, a)>

type spstate<a>
  Ready
    r : proc<a>
  Blocked 
    pid :int 
    proc:proc<a>

type sstate<a> 
  Sstate     
    q : list<(int, spstate<a>)>
    done  : list<(int,a)>
    pid   : int
    pnext : int
\end{lstlisting}


We first define an auxiliary function which abstracts some of the scheduling logic. This function considers three cases; if the next process is ready, it is run, if the next process is blocked it is sent to the back of the queue, if there are no more processes to run, it returns the list of process results.

\begin{lstlisting}
fun runNext<a>(st: sstate<a> ) : <div,co|e> list<(int,a)> 
  match st.q
    Nil -> st.done
    Cons((bpid, Blocked(pid', proc)), ps) -> 
      val newQ = ps ++ [(bpid, Blocked(pid', proc))]
      val newsState =  Sstate (q = newQ, done = st.done, pid = st.pid, pnext = st.pnext)
      runNext(newsState) 
    Cons((rpid, Ready(proc)), ps) -> 
      val newState =  Sstate (q = ps, done = st.done, pid = rpid, pnext = st.pnext)
      val res = proc.p
      res(newState)
\end{lstlisting}

The scheduler is a handler for concurrency effects (\texttt{co} that occur when running the suspended computation comp(). My implementation uses state to keep track of the scheduler state, instead of parameterised handlers in the original Hillerström implementation. 

It handles fork operations by constructing a child process in a Ready state and adding it to the end of the process queue. The Child's resumption is parameterised with 0 so it is aware it is the child process. Finally we parameterise the resumption to the parent with the PID of the child process.

Wait is parameterised by the process it Waits upon. We check the process queue for the process we are waiting for. If it is in the queue then we add this queue in a Blocked state to the end of the process queue. Otherwise we add it in a ready state to the end of the process queue.

Interrupt simply adds the current process to the end of the process queue.

\begin{lstlisting}
fun scheduler<a,e>(comp : () -> <co,div> a, init : sstate<a>) : <div|e> list<(int,a)>
  with handler
    return(x) -> 
      val done'   = Cons((init.pid, x), init.done)
      val newInit = Sstate( q=init.q, done = done', pid = init.pid, pnext= init.pnext)
      runNext(newInit)

    ctl ufork() -> 
      val childProc  = fn(st) ->  resume(0, stChild)  
      val childPid   = init.pnext
      val newQ       =  init.q ++ 
                        [(childPid, Ready(childProc(st)))]
      val st'        = SState (q = newQ, done=init.done, pid=init.pid, pnext = childPid + 1 )
      r(childPid, st')

    ctl sinterrupt() ->
      val interrupted = fn(stInt) -> resume(())
      val newQ        = init.q ++ [(init.pid, Ready(Proc(interrupted)))]
      val _ = runNext(Sstate(q = newQ, done = init.done, pid = init.pid, pnext = init.pnext))
      ()

    ctl wait(pid') ->
      val waitingProc = fn(stWait) -> r((), stWait)
      val newQ =
        if hasPid(pid', init.q)
        then init.q ++ [(init.pid, Blocked(pid', Proc(waitingProc)))]
        else init.q ++ [(init.pid, Ready(Proc(waitingProc)))]
      runNext(Sstate( q = newQ, done = init.done, pid=init.pid,pnext=init.pnext))
      
  comp()
\end{lstlisting}    

\section{Chapter Conclusion}

In this chapter we have shown how effect handler oriented programming provides a clean, modular and composable framework for implementing complex control flow and dynamic scoping problems. Using a Unix-like system as a case study, we demonstrated how features such as file I/O, environment variables, process forking, and scheduling can each be modelled as distinct effects and handled independently. This implementation phase will demonstrate that the same modular handler-based abstractions that aid program design will also support program verification.




\chapter{Reasoning}

In this section we use equational reasoning using algebraic equivalences for deep handlers as defined in Pretnar's Handlers Tutorial \cite{pretnar_introduction_2015}. These are written in figure \ref{fig:equational-laws}.  These equivalences provide a foundation for proving formal properties of Unix programs implemented using effect handlers. If we can apply these rules to transform one computation into another, we can consider them the same up to observational equivalence.
An external observer refers to anything outside the computation itself, including systems like the file system, environment variables, or exception handlers that could observe or be affected by its effects.

Effect handlers make this kind of reasoning possible by making effects explicit and providing a clear separation between where effects are invoked and how they are handled, enabling formal reasoning with algebraic rules.

We aim to verify the correctness of our Unix implementation. Where associated algebraic theories exist for the effects we model, such as state or nondeterminism, we aim to show our implementations satisfies the laws of those theories. In the case where no established equational theory applies, we instead provide reasoning to demonstrate the implementation behaves as intended.


\begin{figure}[H]
    \centering
    \begin{tcolorbox}[colframe=black, colback=white, sharp corners]
    \begin{align*}
        &\text{(1) } \quad \text{do } x \leftarrow \text{return } v \; \text{in } c \equiv c[v/x] \\
        &\text{(2) } \quad \text{do } x \leftarrow \text{op}(v; y.\, c_1) \; \text{in } c_2 \equiv \text{op}(v; y.\, \text{do } x \leftarrow c_1 \; \text{in } c_2) \\
        &\text{(3) } \quad \text{do } x \leftarrow c \; \text{in return } x \equiv c \\
        &\text{(4) } \quad \text{do } x_2 \leftarrow (\text{do } x_1 \leftarrow c_1 \; \text{in } c_2) \; \text{in } c_3 \equiv \text{do } x_1 \leftarrow c_1 \; \text{in } (\text{do } x_2 \leftarrow c_2 \; \text{in } c_3) \\
        &\text{(5) } \quad \text{if true then } c_1 \textbf{ else } c_2 \equiv c_1 \\
        &\text{(6) } \quad \text{if false then } c_1 \textbf{ else } c_2 \equiv c_2 \\
        &\text{(7) } \quad \textbf{if } v \textbf{ then } c[\text{true}/x] \textbf{ else } c[\text{false}/x] \equiv c[v/x] \\
        &\text{(8) } \quad (\lambda x \rightarrow c) \; v \equiv c[v/x] \\
        &\text{(9) } \quad \lambda x \rightarrow v \; x \equiv v 
    \end{align*}
    \end{tcolorbox}
    
    \begin{tcolorbox}[colframe=black, colback=white, sharp corners]
    \begin{align*}
        &\text{(10) } \quad \text{with } h \; \text{handle } (\text{return } v) \equiv c_r[v/x] \\
        &\text{(11) } \quad \text{with } h \; \text{handle } (\text{op}_i(v; y.\, c)) \equiv c_i[v/x, (\lambda y \rightarrow \text{with } h \; \text{handle } c)/k]  (1 \leq i \leq n) \\
        &\text{(12) } \quad \text{with } h \; \text{handle } (\text{op}(v; y.\, c)) \equiv \text{op}(v; y.\, \text{with } h \; \text{handle } c) \quad (\text{op} \notin \{\text{op}_i\}_{1 \leq i \leq n}) \\
        &\text{(13) } \quad \text{with } (\text{handler} \{\text{return } x \rightarrow c_2\}) \; \text{handle } c_1 \equiv \text{do } x \leftarrow c_1 \; \text{in } c_2
    \end{align*}
    \end{tcolorbox}

    \caption{Pretnar's Algebraic Equivalences}
    \label{fig:equational-laws}
\end{figure}





\section{Single-Cell State Proofs}

\subsection{Simplified single-cell state}

\label{subsec:simplified-state}
We begin by reasoning about a minimal model of state, consisting of a single global cell and get and put operations. These handlers were taken from Pretnar's Handler Tutorial \cite{pretnar_introduction_2015}. Although this model is deliberately simple, the reasoning principles it demonstrates are general. We will see that the same basic properties of state we prove here arise in more complex examples later on.  

\begin{figure}[H]
\centering

\[
\textbf{State} := \left\{
\mathsf{get} : 1 \rightsquigarrow S, \quad
\mathsf{put} : S \rightsquigarrow 1
\right\}
\]

\handlerDef{state}{
    &\text{get}(\_, k) \mapsto \text{fun s} \rightarrow (k\ s)\ s \\
    &\text{put}(s, k) \mapsto \text{fun } \_ \rightarrow (k\ () )\ s \\
    &\text{return}\ x \mapsto \text{fun } \_ \rightarrow \text{return } x 
}

\caption{The \textbf{State} effect signature and its interpretation by the \textit{state} handler}
\label{fig:state-handler}
\end{figure}

To briefly explain this handler, the get operation returns a function which is parametrised by the current state \lstinline{s}. It takes this value of state into the continuation \lstinline{k}. This provides a return value to the get operation in the continuation \lstinline{k}. The second \lstinline{s} in the function body rethreads the current state back into the continuation, without this second application, state would not be preserved for subsequent operations. 
The put clause returns a function that takes a parameter, representing the prior state, but immediately discards it. Since we are working within a single-cell state model, the previous state is irrelevant as it is about to be overwritten by the new state value. The continuation is applied to () (as a put operation returns no meaningful result), and the resulting computation is run under the new state \lstinline{s}, thereby establishing \lstinline{s} as the current state for the rest of the computation.

We use this handler to establish four fundamental state laws. These form a basis for reasoning about state, capturing the essential behaviour of get and put, and allowing us to reason algebraically about arbitrary sequences of state operations.

\begin{itemize}
    \item Put-Put 
    \item Put-Get
    \item Get-Put
    \item Get-Get
\end{itemize}

As these proofs take up a lot of space, in the interest of brevity, we will only show one example here. The complete put of proofs are attached in appendice \ref{full-simplified-single-cell-state-proof} 

\subsubsection*{Idempotence of Put}
Performing consecutive \lstinline{put} operations retains only the final update, as each invocation completely overwrites the current state:
\[
\begin{aligned}
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{put} \; a \; (x. \operatorname{put} \; b \; (y. C)) \\
    &\equiv \\
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{put} \; b \; (y. C)
\end{aligned}
\]

\underline{Left Hand Side}


\begin{align*}
&\equiv\quad (11) \\
&\text{fun } \_ \rightarrow (k\ ())\ s\ [a/s,\ \text{fun } x \rightarrow \textbf{with state handle } put(b)(y.C)/k] \\
&\equiv\quad (\text{subst}) \\
&\text{fun } \_ \rightarrow ((\text{fun } x \rightarrow \textbf{with state handle } put(b)(y.C))\ ())\ a \\
&\equiv\quad (8) \\
&\text{fun } \_ \rightarrow (\textbf{with state handle } put(b)(y.C))\ a \\
&\equiv\quad (11) \\
&\text{fun } \_ \rightarrow (\text{fun } \_ \rightarrow (k\ ())\ s\ [b/s, \text{fun } y \rightarrow \textbf{with state handle } C/k])\ a \\
&\equiv\quad (\text{subst}) \\
&\text{fun } \_ \rightarrow (\text{fun } \_ \rightarrow (\text{fun } y \rightarrow \textbf{with state handle } C)\ ())\ b)\ a \\
&\equiv\quad (8) \\
&\text{fun } \_ \rightarrow (\text{fun } \_ \rightarrow (\textbf{with state handle } C)\ b)\ a \\
&\equiv\quad (\text{application}) \\
&\text{fun } \_ \rightarrow (\textbf{with state handle } C)\ b
\end{align*}


\underline{Right Hand Side}
\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (k\ ())\ s\ [b/s, \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C)\ ())\ b \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C)\ b
\end{array}
\]

LHS $\equiv$ RHS



\subsection{Verifying the Implementation of Environment Variables}
This simplified model provides us with a great way to verify the concrete implementation of environment variables in our system. These handlers form an example of single-cell state where the operations Su(user) corresponds to set(x) and Ask() corresponds to get:

\begin{figure}[H]
\centering

\[
 \text{sessionmgr} \langle \mathit{u}, m \rangle \;\overset{\mathrm{def}}{=} \;
 \text{env} \langle \mathit{u}, (\lambda \langle \rangle.\; \textbf{handle}\; m \langle \rangle\; \textbf{with}  \{ 
 \begin{array}{ll}
   \textbf{return}\; res & \mapsto res \\
   \!\langle \text{Su}\; \mathit{u'} \Rightarrow \textbf{resume} \!\rangle & \mapsto \text{env} \langle \mathit{u'}, \textbf{resume} \rangle
 \end{array}
 \} \rangle 
 \]
\[
\begin{array}{l}
\\
\text{env} \langle \mathit{u}, m \rangle \;\overset{\mathrm{def}}{=} \;
\textbf{handle}\; m \langle \rangle\; \textbf{with} \left\{
\begin{array}{ll}
\textbf{return}\; res  \mapsto res \\
\langle \text{Ask} \langle \rangle \Rightarrow \textbf{resume} \rangle  \mapsto \\
\quad\quad\quad\quad\quad\textbf{case}\; \mathit{u}\; \left\{
\begin{array}{ll}
\text{Alice} & \mapsto \textbf{resume}\; \text{"alice"} \\
\text{Bob} & \mapsto \textbf{resume}\; \text{"bob"} \\
\text{Root} & \mapsto \textbf{resume}\; \text{"root"}
\end{array}
\right.
\end{array}
\right\}
\end{array}
\]
\caption{Effect signature and handler implementation for environment variables in Tiny Unix}
\label{fig:env-state-handler}
\end{figure}

State is implemented somewhat differently in this system, instead of being passed as a parameter into each continuation it uses an approach called dynamic binding. The environment is not passed explicitly, instead it is scoped dynamically by the handler itself. This allows dynamic scoping where the same variable can be bound to different values in different contexts in during execution. We will see how this becomes useful later on in the context of process management, but for now we will verify that this implementation follows the same state laws as before.

\begin{itemize}
    \item Su-Su
    \item Su-Ask
    \item Ask-Su
    \item Ask-Ask
\end{itemize}
    

We provide one example here and attach the rest in the appendices \ref{environment-variable-proofs}
\subsubsection*{Switch-user, Switch-user}

sessionmgr<Root, 

\[
\begin{array}{l}
\textbf{sessionmgr} \langle \mathit{user}_i,\; \textbf{Su}(\mathit{user}_j)(x.\;\textbf{Su}(\mathit{user}_k)(y.C)) \rangle \\[5pt]

\quad\equiv\quad (\text{apply}) \\[5pt]
\textbf{with }\text{}\text{env} \langle \mathit{user}_i \rangle\; \textbf{handle} \\
\quad\textbf{with session handle}\; \textbf{Su}(\mathit{user}_j)(x.\;\textbf{Su}(\mathit{user}_k)(y.C)) \\[5pt]

\quad\equiv\quad (11) \\[5pt]
\textbf{with }\text{env} \langle \mathit{user}_i \rangle\; \textbf{handle} \\
\quad\textbf{}\text{env} \langle \mathit{user}',\; m \rangle\; [\mathit{user}_j/\mathit{user}',\; \text{fun } x \rightarrow \textbf{with session handle}\; \textbf{Su}(\mathit{user}_k)(y.C)/m] \\[5pt]

\quad\equiv\quad (\text{subst}) \\[5pt]
\textbf{with }\text{env} \langle \mathit{user}_i \rangle\; \textbf{handle} \\
\quad\textbf{with }\text{env} \langle \mathit{user}_j \rangle\; \textbf{handle} \\
\qquad\textbf{with session handle}\; \textbf{Su}(\mathit{user}_k)(y.C) \\[5pt]

\quad\equiv\quad (11) \\[5pt]
\textbf{with }\text{env} \langle \mathit{user}_i \rangle\; \textbf{handle} \\
\quad\textbf{with }\text{env} \langle \mathit{user}_j \rangle\; \textbf{handle} \\
\qquad\textbf{}\text{env} \langle \mathit{user}',\; m \rangle\; [\mathit{user}_k/\mathit{user}',\; \text{fun } y \rightarrow \textbf{with session handle}\; C/m] \\[5pt]

\quad\equiv\quad (\text{subst}) \\[5pt]
\textbf{with }\text{env} \langle \mathit{user}_i \rangle\; \textbf{handle} \\
\quad\textbf{with }\text{env} \langle \mathit{user}_j \rangle\; \textbf{handle} \\
\qquad\textbf{with }\text{env} \langle \mathit{user}_k \rangle\; \textbf{handle}\; \\
\quad\quad\quad C
\end{array}
\]



\section{Multi-Cell State}


\subsection{Basic Multi-cell State}

This simplified system is less accurate to Unix than Hillerström's "Tiny Unix" but maintains the key functionality (however it loses file linking functionality). From now on we will attempt to reason about how state will interact with other features in our system such as nondeterminism and concurrency. 

This handler for multi-cell state generalises the single-cell model we introduced earlier, replacing the implicit single memory cell with an explicit key-value map. The \lstinline{multiState} handler works in much the same way as the previous one. The main difference is that we expect \lstinline{s} to be a map rather than a single-value. Also note that, in the set case, we use the parameter s, we can no longer discard the current state as we want to update a single key-value pair, not completely overwrite the previous mappings. Both lookup and update are parametrised by the current state, \lstinline{s}.

\[
\text{state} \ \overset{\text{def}}{\equiv} \ \text{handler} \left\{
\begin{array}{ll}
\text{get}(k_0; k) &\mapsto \text{fun } s \mapsto \text{let } v = \text{lookup}(k_0, s) \text{ in } (k\ v)\ s \\
\text{set}(k_0, v; k) &\mapsto \text{fun } s \mapsto \text{let s'} = \text{update}([k_0:=v],s) \text{ in } (k\ ())\ s'\ \\
\text{return } x &\mapsto \text{fun } \_ \mapsto \text{return } x
\end{array}
\right\}
\]


In addition to the properties that held in the single-cell state model, which hold for each key independently, the multicell model allows us to derive additional algebraic properties, the commutativity of operations on disjoint keys. These properties capture the algebraic independence of effects targeting separate parts of the state and are formalised by the following equivalences stated in Pretnar's "The Logic and Handling of
Algebraic Effects" \cite{Pretnar:2010}:
\begin{enumerate}
    \item Disjoint Write-Write Commutativity
    \[set(k1,v1);set(k2,v2) \equiv set(k2,v2);set(k1,v1)\] 
    \item Disjoint Write-Read Independence
        \[set(k1,v1);get(k2) \equiv get(k2);set(k1,v1)\] 
    \item Disjoint Read-Read Commutativity
        \[get(k1);get(k2) \equiv get(k2);get(k1)\] 
\end{enumerate}

These properties are vital as they allow for reasoning about different parts of the system to be treated independently. This kind of modular reasoning is particularly important in the presence of nondeterminism and concurrency, where operations may execute in varying orders.


\subsection{Verifying Tiny Unix's File System}

The \textit{"Single UNIX Specification"} does not provide as algebraic theory that its conforming systems must implement. Instead it relies on textual documentation describing syntax and semantics. However we can still reason about its intended functionality in terms of the mapped state laws above.

\begin{itemize}
    \item Reading/Writing/Opening a file which doesn't exist
    \item File linking breaking disjoint key write commutativity 
    \item Dangling symbolic link
\end{itemize}

Unfortunately attempting to reason about the state model directly in Hillerström's thesis proved to be too complicated in practice. Despite significant effort to apply equational reasoning techniques, each small step in the computation rapidly branched out due to the way different layers of the file system, directories, inodes, and data blocks, depended on one another.Unlike in the simplified examples, reasoning about the serial file system's correctness depended on the interaction between multiple nested data structures, namely; the Directory, the i-node list, the data region. 

Each structure relied on invariants maintained by the others: directory entries had to point to valid i-nodes, i-nodes had to reference valid and allocated data blocks, and all pointers needed to remain consistent throughout execution. This tight coupling made it difficult to reason about each component in isolation.

Performing operations such as file writing or creation involved updating global state which involved multiple layers of functions and handlers including get, put, lookup, modify, fileRW, fopen, fwrite, withDefault etc.The need to thread through and update each of these components made algebraic reasoning impractical.


\section{Process Forking}

Process forking introduces nondeterministic branching into our system which exhibits a number of interesting algebraic properties. By first establishing these properties in isolation we provide a basis for understanding how forking composes with other effects in our system. In subsequent sections we will see that Forking breaks many of the algebraic properties that previously held in sequential contexts.

Whilst the algebraic structure of forking is defined in the effect handler, the operational semantics is defined by our scheduler. As the scheduler determines the execution order, from the perspective of a single process, the system appears non-deterministic; it cannot know when or how other processes will affect the overall state of the system. \textcolor{red}{The scheduler can adopt different policies e.g. fairness, liveness etc which can affect the execution order which are outwith our algebra.}


\subsection{Equational Theories of Non Determinism}
Abstract notions of nondeterminism state that is is modelled by an abstract algebraic structure known as the \textit{semi-lattice} laws \cite{Pretnar:2010} which is governed by the following algebraic properties:

\begin{figure}[H]
\centering
\[
\begin{array}{ll}
  A \oplus A \equiv A & \text{(Idempotency)} \\
  A \oplus B \equiv B \oplus A & \text{(Commutativity)} \\
  A \oplus (B \oplus C) \equiv (A \oplus B) \oplus C & \text{(Associativity)}
\end{array}
\]
\caption{%
  Equational theory of nondeterminism.
}
\end{figure}





\textcolor{red}{However, this abstraction becomes problematic when one attempts to model or implement concrete algorithms that resolve nondeterministic choices, such as our scheduler.  the symmetry and idempotency assumed by semi-lattices is too strong to hold. Associativity holds in the presence of any other effect \cite{NotByEquationsAlone}}



\subsection{Properties of Tiny Unix Forking}


\begin{figure}[H]
\centering
\[
\text{nondet} \ \overset{\text{def}}{\equiv} \ \text{handler} \left\{
\begin{array}{ll}
\text{Fork}() &\mapsto \text{\textbf{resume} True ++ \textbf{resume} False} \\
\textbf{return } x &\mapsto \textbf{return } [x]
\end{array}
\right\}
\]
\caption{Tiny Unix's implementation of nondeterminism}
\label{nondetImplementation}
\end{figure}


Unfortunately, in Hillerström's implementation of nondeterminism \ref{nondetImplementation}, forking is neither commutative nor associative. This makes sense for idempotency as two identical processes are clearly not equivalent to one process. Commutativity is not valid either as the order in which the processes are returned could affect observational behaviour when under scheduling.

\textcolor{red}{say we run in to our first problem suggested by "Not by Equations Alone", equational theories often exert unrealistic demands on their implementations, they cite nondeterminism as an example of this}


However we can show that, in the absence of other effects, forking is associative. In this context, the order in which fork operations are grouped does not affect the set of possible execution paths. The observable behaviour remains unchanged remains the same, regardless of how the forks are nested. The code below demonstrates this in practice.

\vspace{-2em}
\begin{figure}[H]
    \centering


\begin{table}[H]
\centering
\begin{tabular}{p{0.45\textwidth} c p{0.45\textwidth}}
\begin{lstlisting}
if Fork(){
    if Fork(){
        A
    }
    else{
        B
    }
}
else{
    C
}
\end{lstlisting}
&
&
\begin{lstlisting}
if Fork(){
    A
} 
else{
    if Fork(){
        B
    }
    else{
        C
    }
}
\end{lstlisting}
\end{tabular}
\end{table}
\vspace{-2em}

    \caption{Equivalent code structures under associative forking}
    \label{fig:enter-label}
\end{figure}




Structural properties describe how computations may be grouped and rearranged without changing the meaning of a program. This makes them especially useful for restructuring code, as they allow us to transform the shape of a program without affecting its behaviour. In particular, associativity enables us to restructure nested control flows, revealing opportunities for parallel execution. This facilitates tree-based parallel execution, allowing nested computations to be restructured into balanced, parallel-friendly forms.

For example, if we knew that workers were independent of each other, restructuring the following code:
\vspace{-1em} % reduce space above table
\begin{figure}[H]
    \centering
    \begin{tabular}{p{0.45\textwidth} c p{0.45\textwidth}}
        \begin{lstlisting}
Server forks Worker 1
Server forks Worker 2
Server forks Worker 3
        \end{lstlisting}
        &
        &
        \begin{lstlisting}
Server forks Worker 1
    Worker 1 forks Worker 2
        Worker 2 forks Worker 3
        \end{lstlisting}
    \end{tabular}
    \vspace{-1em} % reduce space below table

    \caption{Equivalent flat and nested process structures.}
    \label{fig:process-restructuring}
\end{figure}



\underline{Proof}

In the continuation passing style, we  write the associativity property of \lstinline{nondet} as:
\equivalenceStatement{nondet}
{
if Fork() then
if Fork() 
a 
else 
b 
else 
c
}
{nondet}
{
if Fork() then a 
else 
if Fork() 
 then b 
 else c
}

\underline{Left Hand Side}
\[
\begin{array}{l}
\quad \equiv \quad (11) \\[5pt]
\textbf{resume }\text{True} \;\texttt{++}\; \textbf{resume } \text{False} \; [()/x,\; \text{fun } r \rightarrow \textbf{with } \text{nondet handle } (\textbf{if } r \textbf{ then } a \\ \text{ else Fork()}(s \rightarrow \textbf{if }  s \textbf{ then } b \textbf{ else } c)) \;/\; k] \\[5pt]

\quad \equiv \quad (\text{subst}) \\[5pt]
\text{fun } r \rightarrow \textbf{with } \text{nondet handle } (\textbf{if } r \textbf{ then } a \text{ else Fork()}(s \rightarrow \textbf{if } s \textbf{ then } b \textbf{ else } c)) \;\text{True} \\[2pt]
\texttt{++} \\ 
\text{fun } r \rightarrow \textbf{with } \text{nondet handle } (\textbf{if } r \textbf{ then } a \text{ else Fork()}(s \rightarrow \textbf{if } s \textbf{ then } b \textbf{ else } c)) \;\text{False} \\[5pt]

\quad \equiv \quad (8,\; \text{evaluate}) \\[5pt]
\textbf{with } \text{nondet handle } a \;\texttt{++}\; \textbf{with } \text{nondet handle } \text{Fork()}(s \rightarrow \textbf{if } s \textbf{ then } b \textbf{ else } c) \\[5pt]

\quad \equiv \quad (11) \\[5pt]
a \;\texttt{++}\; (\text{resume True} \;\texttt{++}\; \text{resume False} \; [()/x,\; \text{fun } s \rightarrow \textbf{if } s \textbf{ then } b \textbf{ else } c \;/\; k]) \\[5pt]

\quad \equiv \quad (\text{subst}) \\[5pt]
a \;\texttt{++}\; (\text{fun } s \rightarrow \textbf{if } s \textbf{ then } b \textbf{ else } c) \;\text{True} \\
\texttt{++} \\
(\text{fun } s \rightarrow \textbf{if } s \textbf{ then } b \textbf{ else } c) \;\text{False} \\[5pt]

\quad \equiv \quad (8,\; \text{evaluate}) \\[5pt]
a \;\texttt{++}\; (b \;\texttt{++}\; c)
\end{array}
\]


\underline{Right Hand Side}



\[
\begin{array}{l}
\quad \equiv \quad (11) \\[5pt]
\textbf{resume } \text{True} \;\texttt{++}\; \textbf{resume } \text{False} \; [()/x,\; \text{fun } r \rightarrow \textbf{with } \text{nondet handle } (\textbf{if } r \text{ then Fork()} \\(s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b) \textbf{ else } c) \;/\; k] \\[5pt]

\quad \equiv \quad (\text{subst}) \\[5pt]
\text{fun } r \rightarrow \textbf{with } \text{nondet handle } (\textbf{if } r \text{ then Fork()}(s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b) \textbf{ else } c) \;\text{True} \\
\texttt{++} \\
\text{fun } r \rightarrow \textbf{with } \text{nondet handle } (\textbf{if } r \text{ then Fork()}(s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b) \textbf{ else } c) \;\text{False} \\[5pt]

\quad \equiv \quad (8,\; \text{evaluate}) \\[5pt]
\textbf{with } \text{nondet handle } \text{Fork()}(s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b) \;\texttt{++}\; \textbf{with } \text{nondet handle } c \\[5pt]

\quad \equiv \quad (11) \\[5pt]
(\textbf{resume } \text{True} \;\texttt{++}\; \textbf{resume } \text{False} \; [()/x,\; \text{fun } s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b \;/\; k]) \;\texttt{++}\; c \\[5pt]

\quad \equiv \quad (\text{subst}) \\[5pt]
(\text{fun } s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b)\; \text{True} \;\texttt{++}\; (\text{fun } s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b)\; \text{False} \;\texttt{++}\; c \\[5pt]

\quad \equiv \quad (8,\; \text{evaluate}) \\[5pt] 
(\text{fun } s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b)\; \text{True} \;\texttt{++}\; (\text{fun } s \rightarrow \textbf{if } s \textbf{ then } a \textbf{ else } b)\; \text{False} \;\texttt{++}\; c \\[5pt]

\quad \equiv \quad (\text{evaluate}) \\[5pt]
(a \;\texttt{++}\; b) \;\texttt{++}\; c
\end{array}
\]


\[
\textbf{LHS} = \textbf{RHS} \quad \text{and} \quad a ++ (b ++ c) \equiv (a ++ b) ++ c
\]




\section{State Models and Handler Composition}
In the previous sections we reasoned about state in a  sequential setting and introduced nondeterminism through process forking. We now turn to the question of how these two effects interact when composed. The order of composition of the state and nondeterminism handlers, significantly impacts the semantics of stateful computations. \textcolor{red}{describe in terms of monads? Say much work has been done in this area. }

We obtain two different models of state depending on this ordering \textcolor{red}{we must define local and global state formally as these are loaded terms}:
\begin{itemize}
    \item Local State (outer nondeterminism, inner state): Here each nondeterministic branch maintains its own independent state, akin to local variables within a program. The scope is private to the process and only lasts as long as the process lasts.
    \item Global State (outer state, inner nondeterminism): Here the same state is shared among all branches of a nondeterministic computation similar to the case of a global file system.
\end{itemize}



We now go onto describe the properties of global and local state as described in the latest version of the Open Group Unix Specifications \cite{posix}. Our goal is to show how these behaviours are captured within our effect handler model, and to contrast this with the conventional imperative mechanisms typically used to implement such functionality in Unix-like systems. \textcolor{red}{explain these in terms of the algebraic properties of local/global state described in papers}


\subsection{Local State : Unix Specifications}

\textcolor{red}{As local state, we are referring to  state that is scoped to a single process and is not observable or accessible by any other process in the system. Examples include process-local variables, file descriptor tables, or other private data that do not persist across process boundaries or influence other concurrent executions. 
}



\subsubsection{Independent and Deterministic State} 

In From High to Low: Reasoning About Effect Implementations \cite{hightoLow}, Wang and Schrijvers show that local state, when composed with nondeterminism using the \lstinline{StateT} monad transformer \cite{jones1995}, preserves the associativity and identity laws of the state monad. This ensures that state remains deterministic within each nondeterministic branch.

In this handler configuration, the state handler is applied to each branch, allowing each to maintains a separate, independent state. This isolation ensures that there is no interference between parallel computations. Within each branch, the state is deterministic, the state laws shown in previous sections hold.

\begin{tcolorbox}[colback=gray!10, colframe=gray!60, sharp corners, boxrule=0.5pt, title={POSIX Base Specifications, Issue 7, p.899}]
    Any modifications to the data in
 MAP\_PRIVATE mappings made by the parent after fork() returns shall be visible only to
 the parent. Modifications to the data in MAP\_PRIVATE mappings made by the child shall
 be visible only to the child.
  \\ ... \\
A process is an address space with one or more threads executing within that address space. Each process has its own private memory and system resources
\end{tcolorbox}

We can see, directly from handler structure, when rewriting these laws in handler notation, that they remain trivially valid: the state handler processes all effects before any branching occurs.
\begin{figure}[H]

\centering

\vspace{-0.0em}
\[
\begin{array}{@{}l@{}l@{}l@{}l@{}}
\shortstack{
  \textbf{with } \text{nondet handle} \\
  \textbf{with } \text{state handle} \\
  \quad\quad\text{set(a)(x.set(b)(y.C))}
}
&
\shortstack{
  \textbf{with } \text{nondet handle} \\
  \textbf{with } \text{state handle} \\
  \quad\quad\text{set(a)(x.get()(y.C))}
}
&
\shortstack{
  \textbf{with } \text{nondet handle} \\
  \textbf{with } \text{state handle} \\
  \quad\quad\text{get()(x.set(b)(y.C))}
}
&
\shortstack{
  \textbf{with } \text{nondet handle} \\
  \textbf{with } \text{state handle} \\
  \quad\quad\text{get()(x.get()(y.C))}
}
\end{array}
\]
\caption{Standard state laws remain valid under local state, as all state operations are resolved before nondeterminism is introduced.}
\label{fig:local-state-laws}
\end{figure}





\begin{longtable}{@{}l@{}}
\textbf{with } nondet \textbf{ handle} \\ 
\quad \textbf{with } state \textbf{ handle} \\
\quad\quad \text{Fork()(p.if p then set(x)(a.C\textsubscript{1}) else set(y)(b.C\textsubscript{2}))} \\
\quad$\equiv$ (12) \\[5pt]
\textbf{with } nondet \textbf{ handle} \\
\quad \text{Fork()}(a.\ \textbf{with } state \textbf{ handle } (\textbf{if } a \text{ then set(x)}(b.C\textsubscript{1}) \text{ else set(y)}(d.C\textsubscript{2}))) \\
\quad$\equiv$ (11) \\[5pt]
\textbf{resume } x \text{ ++ } \textbf{resume } y \\
\quad [()/v,\ \text{fun } a $\rightarrow$ \textbf{with } nondet,\ state \textbf{ handle if } a \text{ then set(x)}(b.C\textsubscript{1}) \text{ else set(y)}(d.C\textsubscript{2})] \\
\quad$\equiv$ (subst) \\[5pt]
\text{fun } a $\rightarrow$ \textbf{with } nondet,\ state \textbf{ handle if } a \text{ then set(x)}(b.C\textsubscript{1}) \text{ else set(y)}(d.C\textsubscript{1})\ x \\
\text{++} \\
\text{fun } a $\rightarrow$ \textbf{with } nondet,\ state \textbf{ handle if } a \text{ then set(x)}(b.C\textsubscript{1}) \text{ else set(y)}(d.C\textsubscript{2})\ y \\
\quad$\equiv$ (8, simplify) \\[5pt]
\textbf{with } nondet,\ state \textbf{ handle set(x)}(b.C\textsubscript{1}) \\
\text{++} \\
\textbf{with } nondet,\ state \textbf{ handle set(y)}(d.C\textsubscript{2}) \\
\quad$\equiv$ (11) \\[5pt]
\text{fun } \_ $\rightarrow$ (k ())\ s\ [\text{x}/s,\ \text{fun } b $\rightarrow$ \textbf{with } nondet,\ state \textbf{ handle }(c.C\textsubscript{1})] \\
\text{++} \\
\text{fun } \_ $\rightarrow$ (k ())\ s\ [\text{y}/s,\ \text{fun } d $\rightarrow$ \textbf{with } nondet,\ state \textbf{ handle }(e.C\textsubscript{2})] \\
\end{longtable}




\subsubsection{Impermanence} 
The placement of the state handler inside the nondeterminism handler ensures that state is scoped to each branch individually. Once a branch terminates, its state is no longer stored.




\begin{tcolorbox}[colback=gray!10, colframe=gray!60, sharp corners, boxrule=0.5pt, title={POSIX Base Specifications, Issue 7, p.554}]
Memory mappings that were created in the process shall be unmapped before the process
 is destroyed.
 \end{tcolorbox}
 

\begin{longtable}{@{}l@{}}
\textbf{with } nondet handle \\
\quad \textbf{with } state handle \\
\quad\quad set(a)(x. return 0) \, ++ \, set(b)(y. return 0) \\

\quad$\equiv$ (11) \\

\textbf{with } nondet handle \\
\quad fun \_ $\rightarrow$ (k())s \ [a/s,\ fun y $\rightarrow$ \textbf{with } state handle return 0]\\
\quad\quad ++ \\
\quad fun \_ $\rightarrow$ (k())s \ [b/s,\ fun y $\rightarrow$ \textbf{with } state handle return 0] 
\quad$\equiv$ (subst) \\
\textbf{with } nondet handle \\
\quad fun \_ $\rightarrow$ (fun y $\rightarrow$ \textbf{with } state handle return 0)() \ a \\
\quad\quad ++ \\
\quad fun \_ $\rightarrow$ (fun y $\rightarrow$ \textbf{with } state handle return 0)() \ b \\
\quad$\equiv$ (8) \\
\textbf{with } nondet handle \\
\quad fun \_ $\rightarrow$ (\textbf{with } state handle return 0) \ a \\
\quad ++ \\
\quad fun \_ $\rightarrow$ (\textbf{with } state handle return 0) \ b \\
\quad$\equiv$ (10) \\
\textbf{with } nondet handle \\
\quad fun \_ $\rightarrow$ 0 \ a \quad ++ \quad fun \_ $\rightarrow$ 0 \ b \\
\quad$\equiv$ (apply) \\
\textbf{with } nondet handle \\
\quad 0 ++ 0 \\
\end{longtable}

    
\subsubsection{State Inheritance:} When a process is forked, the child receives a snapshot of the parent’s state at that moment. This fork-time duplication ensures that both processes start from identical conditions, but proceed independently. Changes made in one process no longer affect the other.

\textcolor{red}{show how get and set distribute over the fork (put/get left distributivity)}

\textcolor{red}{show get;return x=  x and set(a); return x = x}

\begin{tcolorbox}[colback=gray!10, colframe=gray!60, sharp corners, boxrule=0.5pt, title={POSIX Base Specifications, Issue 7, p.897}]
Memory mappings created in the parent shall be retained in the child process
\end{tcolorbox}


\begin{tcolorbox}[examplebox, title=Practical Example: Inheriting environment variables]


\begin{lstlisting}
sessionmgr(Root, {
  if Fork() then {
    su(Alice);
    whoami();      // prints "Alice"
  } else {
    whoami();      // prints "Root"
})});
...
\end{lstlisting}
\[
\begin{array}{l}
\textbf{with } \text{nondet} \textbf{ handle } \\
\quad \textbf{with } \text{sessionmgr(Root)} \textbf{ handle } \\
\quad\quad \text{Fork}()(r. \textbf{if } r \textbf{ then } (su(Alice); \textit{whoami}()) \textbf{ else } \textit{whoami}()) \\[5pt]
\quad\equiv\quad (12) \\
\end{array}
\]

\begin{longtable}{@{}l@{}}
\textbf{with } nondet \textbf{ handle } Fork()(r. \textbf{with } sessionmgr \textbf{ handle } (\textbf{if } r \textbf{ then } (su(Alice); \\ whoami()) \textbf{ else } whoami())) \\

\quad$\equiv$\quad (11) \textit{The state handler is applied outside, so all branches share the same initial state.} \\
\textbf{resume } True ++ \textbf{resume } False \\
\quad [ ()/x,\ \textbf{fun } r $\rightarrow$ \textbf{with } sessionmgr \textbf{ handle } (\textbf{if } r \textbf{ then } (su(Alice); whoami()) \textbf{ else } whoami()) / k ] \\

\quad$\equiv$\quad (subst) \\
\textbf{fun } r $\rightarrow$ \textbf{with } sessionmgr(Root) \textbf{ handle } (\textbf{if } r \textbf{ then } (su(Alice); whoami()) \textbf{ else } whoami())\ True \\
++ \\
\textbf{fun } r $\rightarrow$ \textbf{with } sessionmgr(Root) \textbf{ handle } (\textbf{if } r \textbf{ then } (su(Alice); whoami()) \textbf{ else } whoami())\ False \\

\quad$\equiv$\quad (8) \\
\textbf{with } sessionmgr \textbf{ handle } (su(Alice); whoami()) \\
++ \\
\textbf{with } sessionmgr(Root) \textbf{ handle } whoami() \\

\quad$\equiv$\quad (11) \\
\textbf{env}(x, k)\ [\text{``Alice''}/x,\ \textbf{fun } y $\rightarrow$ \textbf{with } sessionmgr \textbf{ handle } whoami()/k] \\
++ \\
z $\leftarrow$ \textbf{do Ask()} \ [()/x,\ \textbf{fun } z $\rightarrow$ \textbf{with } env \textbf{ handle } C/k] \\
\end{longtable}


\end{tcolorbox}








\subsection{Global State : Unix Specifications}

\textcolor{red}{As global state, we are referring to memory or system resources that are shared across all processes and remain accessible regardless of process boundaries. Examples include the file system, shared memory mappings, or other kernel-managed structures that persist independently of any single process. This state is visible to and modifiable by all concurrently running processes, meaning changes made in one process can influence the behaviour or outcomes of others.
}

\textcolor{red}{
Global state lacks many of the algebraic properties we observed in the local case. Algebraic effect systems describe the structure of programs, but not the sequence in which operations occur. As local state is indepenent and commutative this didn't matter, but in global state this leads to the race conditions described earlier \cite{raceConditionRef}
}

\textcolor{red}{Our algebra defines which operations are performed and how they are composed structurally, but they don't have a notion of time/order and so don't describe operational runtime behaviour. We rely on external ordering mechanisms such as scheduling and mutexes to define how these are actually executed.
}

\textcolor{red}{Global state cannot be described as a monad in this setting, because it lacks a coherent distributive interaction with nondeterminism. The absence of this law means that basic monadic composition, such as associativity and bind chaining}

The Unix specifications note this problem and delegates the problem to the application level. We talk about these process synchronisation methods later in this chapter.


\begin{tcolorbox}[colback=gray!10, colframe=gray!60, sharp corners, boxrule=0.5pt, title={POSIX Base Specifications, Issue 7, p.2316}]
 This volume of POSIX.1-2008 does not specify the behaviour of concurrent writes to a regular file |
 from multiple threads, except that each write is atomic (see Section 2.9.7, onpage 522). |
 Applications should use some form of concurrency control.
\end{tcolorbox}

We illustrate how this non-deterministic behaviour arises by reasoning through handler scope and equational equivalence.

\begin{longtable}{@{}l@{}}
\textbf{with } state \textbf{ handle} \\
\quad \textbf{with } nondet \textbf{ handle} \\
\quad\quad Fork()(p.\textbf{if } p \textbf{ then } set(x)(a.C\textsubscript{1}) \textbf{ else } set(y)(b.C\textsubscript{2})) \\
\quad$\equiv$ (11) \\
\\
\textbf{with } state \textbf{ handle} \\
\quad \textbf{resume } True \text{ ++ } \textbf{resume } False \\
\quad [()/v,\ \text{fun } r $\rightarrow$ \textbf{with } nondet \textbf{ handle if } r \textbf{ then } set(x)(a.C1) \textbf{ else } set(y)(b.C2)] \\
\\
\quad$\equiv$ (\text{subst}) \\
\textbf{with } state \textbf{ handle} \\
\quad fun r $\rightarrow$ \textbf{with } nondet \textbf{ handle if } r \textbf{ then } set(x)(a.C1) \textbf{ else } set(y)(b.C2)\ True \\
\quad ++ \\
\quad fun r $\rightarrow$ \textbf{with } nondet \textbf{ handle if } r \textbf{ then } set(x)(a.C1) \textbf{ else } set(y)(b.C2)\ False \\
\\
\quad$\equiv$ (8) \\
\textbf{with } state \textbf{ handle} \\
\quad \textbf{with } nondet \textbf{ handle if } True \textbf{ then } set(x)(a.C1) \textbf{ else } set(y)(b.C2) \\
\quad ++ \\
\quad \textbf{with } nondet \textbf{ handle if } False \textbf{ then } set(x)(a.C1) \textbf{ else } set(y)(b.C2) \\
\\
\quad$\equiv$ (5,6) \\
\textbf{with } state \textbf{ handle} \\
\quad \textbf{with } nondet \textbf{ handle } set(x)(a.C1) \\
\quad ++ \\
\quad \textbf{with } nondet \textbf{ handle } set(y)(b.C2) \\
\\
\quad$\equiv$ (12) \\
\textbf{with } state \textbf{ handle} \\
\quad set(x)(a.\textbf{with } nondet \textbf{ handle } C1) \\
\quad ++ \\
\quad set(y)(b.\textbf{with } nondet \textbf{ handle } C2) \\
\end{longtable}


This creates a non deterministic outcome, as we can see from the example above, the order in which the scheduler chooses to run these branches will affect the final state of the system. To each process the system is nondeterministic as it doesn't know what state the system will be in when it is run. 




\section{Properties of Processes}

Having clarified the algebraic properties of individual effects and the properties of global and local scope depending on handler ordering, we now look at what properties processes maintain when combining  non determinism, global state, local state.

In this section, we also introduce the exit effect, a non-resumable control operation that disrupts sequencing, and explore how its interaction with other handlers further affects compositional reasoning.


\subsubsection*{Local State Breaks Associativity}

In the purely nondeterministic setting, forking is associative, i.e. the grouping/nesting does not affect its semantics. However in the 

\textcolor{red}{According to most equational theories nondeterminism is supposed to be associative in }

\begin{table}[H]
\centering
\begin{tabular}{p{0.45\textwidth} c p{0.45\textwidth}}
\begin{lstlisting}
if Fork(){
    // A
    if Fork(){
        // A continued
        pass
    }
    else{
        // C
    }
}
else{
    set("B")
}
\end{lstlisting}
&
&
\begin{lstlisting}
if Fork(){
    A
} 
else{
    // B
    set("B");
    if Fork(){
        // B continued
        pass
    }
    else{
        // C
        C
    }
}
\end{lstlisting}
\end{tabular}
\end{table}
\vspace{-2em}


\subsubsection*{Local State Full Distributivity}
Whilst left distributivity does not hold in general, it does hold in the special case of idempotent effects, where repeated applications of R are equivalent to applying it once. As right distributivity holds trivially through algebraicity of sequencing Local State is fully distributive 

\vspace{-2em}
\begin{table}[H]
\centering
\begin{tabular}{p{0.45\textwidth} c p{0.45\textwidth}}
\begin{lstlisting}
set(x);
if Fork(){
    P;
}
else{
    Q;
}
\end{lstlisting}
&
&
\begin{lstlisting}
if Fork(){
    set(x); 
    P;
}
else{ 
    set(x); 
    Q;
}
\end{lstlisting}
\end{tabular}
\end{table}
\vspace{-2em}


\subsection{Summary of Properties Under Composition}


\section{Exceptions}
Processes can terminate successfully by running to completion, or they can terminate by performing an exit system call. The exit call takes a numeric argument indicating the exit status of the process. Unlike other operations exit does not allow for continuation, it discards the remaining computation and terminates execution immediately.

Exceptions disrupt all of the algebraic properties that held before hand, these properties now only hold on the assumption that there is no exception in their scope. Exceptions only hold one property, that any code that succeeds them is semantically irrelevant.


\effectDef{exit n}{\text{Exit : Int} \rightarrow 0}

\[
\mathrm{status} \;\overset{\mathrm{def}}{=}\;
\mathrm{handler} \;\left\{
\begin{array}{ll}
  \mathrm{\textbf{return}\:\_} & \mapsto 0 \quad\\[0.5ex]
  \langle\!\langle \mathrm{Ask} \langle\rangle\rangle &  \mapsto n \quad\\[0.5ex]
 
\end{array}
\right\}
\]

\subsection*{Absorption}

The absorption property states that once an exit operation is called within a computation, any further computation is irrelevant. The following formalises the property that, whatever comes after the exit, does not affect the behaviour of the program:

\equivalenceStatement{status}{Exit (n)(y.C)}{status}{Exit(n)(y.D)}

Because Exit n returns type 0, the uninhabited type, it cannot yield a result. To embed it in a computation expecting a value of type $\alpha$, we use the absurd function to coerce from the empty type to $\alpha$. This reflects that any code after Exit n is unreachable, and ensures the expression remains well-typed.

\[
\mathrm{exit\:n} \;\overset{\mathrm{def}}{=}\;
\mathrm{handler} \;\left\{
\begin{array}{l}
  \text{absurd\:(\textbf{do}\:Exit\:n)} \quad\\[0.5ex]

\end{array}
\right\}
\]

\underline{Left Hand Side}

\[
\begin{array}{l}
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{absurd (do Exit } n) (y. C) \\[5pt]
\quad\equiv\quad (11) \\[5pt]
v [n/v, \text{fun } y \rightarrow \text{\textbf{with} status \textbf{handle} } C/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
n
\end{array}
\]

\underline{Right Hand Side}

\[
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{absurd (do Exit } n) (y. D) \\[5pt]
\quad\equiv\quad (11) \\[5pt]
v [n/v, \text{fun } y \rightarrow \text{\textbf{with} status \textbf{handle} } D/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
n
\end{array}
\]


\section{Process Synchronisation}


So far we have introduced a basic time-sharing, using interrupts and the scheduler, that allows the interleaving of process execution. However, as mentioned before, the way that processes are interleaved is non-deterministic, in other words processes don't know the order in which the will be executed. 

This is a problem because, as we have demonstrated, many of our effects are non-commutative meaning that the order of execution is important for correctness. This creates what is known as a "race condition", A race condition arises when the correctness of a program depends on the nondeterministic interleaving of concurrent operations on shared state. Without explicit synchronisation, the outcome is unpredictable and may differ across runs, making correct behaviour accidental rather than guaranteed.

Currently the only way to guarantee execution order is to execute all the steps, in the same process, sequentially with no interrupts which defeats the point of our time sharing system as no interleaving is possible in this time, which blocks the other processes.

To introduce flexibility around process interleaving whilst maintaining guarantees about execution ordering we introduce two features

\textcolor{red}{Write about the problem of reasoning about the scheduler, something to do with mutually parameterised data types or smn}


\begin{itemize}
    \item \lstinline{Wait} \textbf{system calls: } This allows a parent to suspend execution until its child process (identified by its process id) has finished executing. Think of this as introducing a causal dependency, the parent can only when the child finishes.
    \item \textbf{Mutexes: } This a process to lock a resource, meaning that whilst the process holds this lock, no other process can alter this resource. This guarantees that between interrupts other processes don't alter the resource, which could potentially lead to file corruption. 
\end{itemize}

Combining these two properties by;
\begin{enumerate}
    \item Locking the resource, to prevent other processes simultaneously writing to (and thereby corrupting) our resource
    \item Splitting up the responsibility for writes between its children, and waiting in order for them to complete
    \item Unlocking the resource to allow other processes to access it
\end{enumerate}

we can provide guarantees about write safety whilst still allowing for concurrency.
 

\underline{Scenario 1} 
The 


Reasoning about the scheduler is very difficult because it introduces non-determinism into our system. Unlike the algebraic effects, which have clear equational rules, the behaviour of the scheduler depends on the runtime state of the process queue and the particular order in which processes are resumed. 








\section{Reasoning Conclusions}

In this section we have demonstrated how to use algebraic equivalences to reason about effect handlers. We first examined different models of state depending on how we composed the nondeterminism and state handlers, which corresponded to global file system state and process local state. We verified that each of these models fulfilled the properties that Unix expected them to have. 

We then went on to reason about other interesting properties that the equivalences allowed us to show. This helped to verify that certain program transformations are valid and allowed us to simplify our code. 

We also discovered that many of the rules that held in an isolated setting break down under composition. 

It is very difficult to reason about any properties which rely on the scheduler. This makes reasoning about any forms of process synchronsiation or interprocess communication very difficult to do. Unfortunately effects like \lstinline{Wait} or \lstinline{Acquire} \lstinline{Release} which introduce partial order semantics can't be easily reasoned about using our equivalences as they depend upon the dynamic execution state of our system.

\chapter{Conclusions}

\section{Unix Implementation in Koka}
We have successfully realised the "Toy Unix" system described in Hillerström's paper, we provided implementations for exception handling, a basic IO scheme, user functionality, a basic serial file system, piping and two methods of process scheduling. We achieved the goal we set out to do namely, to show that we can implement a simulation of Unix, tackling the complex problems this presents, in a concise and modular way.

\subsection*{Future Work}
Implementing the process synchronisation features described in the previous chapter would provide vital functionality to our Unix system. Currently, if the system wants to perform writes in order, it can't do any interleaving, meaning that all writes are blocking defeating the purpose of a time-sharing system. Mutexes would provide a simple way to guarantee that resources can't be tampered with.

\section{Reasoning}
We successfully verified that our implementation of Unix follows the rules expected of it according to the Unix Specifications. We quickly ran into problems when trying to reason about anything which was not directly related to code structure, the dynamic state of the scheduler prevented us doing any reasoning about synchronisation or IPC.
\subsection*{Future}
Further reasoning about partial ordering of effectful operations using Waits and Mutexes. We could also implement a model of Local File Descriptor tables for processes and a global Open File Table. This would open up reasoning about sharing and isolation of state and how this aids modularity and functionality.  

% \bibliographystyle{plain}
\bibliographystyle{plain}
\bibliography{mybibfile}


% You may delete everything from \appendix up to \end{document} if you don't need it.
\appendix

\chapter{Single-cell state proofs} 

\section{Simplified State} \label{full-simplified-single-cell-state-proof}

\subsubsection*{Idempotence of Set}
Performing consecutive \lstinline{set} operations retains only the final update, as each invocation completely overwrites the current state:
\[
\begin{aligned}
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{set} \; a \; (x. \operatorname{set} \; b \; (y. C)) \\
    &\equiv \\
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{set} \; b \; (y. C)
\end{aligned}
\]

\underline{Left Hand Side}

\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (k\ ())\ s\ [a/s,\ \text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } set(b)(y.C)/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } set(b)(y.C))\ ())\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } set(b)(y.C))\ a \\[5pt]
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (\text{fun }\_ \rightarrow (k\ ())\ s\ [b/s, \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C/k])\ a \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow (\text{fun }\_ \rightarrow (\text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C)\ ())\ b)\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C)\ b)\ a \\[5pt]
\quad\equiv\quad (\text{application}) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C)\ b
\end{array}
\]

\underline{Right Hand Side}
\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (k\ ())\ s\ [b/s, \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C)\ ())\ b \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C)\ b
\end{array}
\]

LHS $\equiv$ RHS

\subsubsection*{Idempotence of Get}
Reading the state multiple times without an intervening \lstinline{set} yields the same result each time. Repeating a get operation without modifying the state in between has no observable effect on the computation.


\[
\begin{aligned}
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{get}() \left( \mathsf{x.get}() \left( y.C \right) \right) \\
    &\equiv \\
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{get}() \left( x.C[x/y] \right)
\end{aligned}
\]
\underline{Left Hand Side}

\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun } s \rightarrow (k\ s)\ s\ [()/v, \text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } get()(y.C)/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun } s \rightarrow ((\text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } get()(y.C))\ s)\ s \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun } s \rightarrow (\text{\textbf{with} state \textbf{handle} } get()(y.C[s/x]))\ s \\[5pt]
\quad\equiv\quad (11) \\[5pt]
\text{fun } s \rightarrow ((\text{fun } s'\rightarrow (k\ s')\ s')\ [()/v, \text{fun } y\rightarrow \text{\textbf{with} state \textbf{handle}}/k])\ s \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun } s \rightarrow ((\text{fun } s' \rightarrow (( \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C[s/x])\ s')\ s')\ s \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun } s \rightarrow (\text{fun } s' \rightarrow (\text{\textbf{with} state \textbf{handle} } C[s/x][s'/y]s')) \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun } s \rightarrow (\text{\textbf{with} state \textbf{handle} } C[s/x][s/y])\ s
\end{array}
\]



\underline{Right Hand Side}

\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun } s \rightarrow (k\ s)\ s\ [()/v, C[x/y]] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun } s \rightarrow ((\text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } C[x/y])) \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun } s \rightarrow (\text{\textbf{with} state \textbf{handle} } C[x/y][s/x])\ s
\end{array}
\]

LHS $\equiv$ RHS



\subsubsection*{Set-Then-Get}
Reading the state immediately after setting it yields the value that was just written. This property ensures that state updates are immediately observable.
\[
\begin{aligned}
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{set} \; a \; (x. \operatorname{get}() \; (y. C)) \\
    &\equiv \\
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{set} \; a \; (x. C[a/x])
\end{aligned}
\]

\underline{Left Hand Side}
\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (k\ ())\ s\ [a/s, \text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } get() (y. C)/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } get() (y. C)) ())\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } get() (y. C))\ a \\[5pt]
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (\text{fun } s' \rightarrow  (k\ s')\ s' [()/s, \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C / k])\ a \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } s' \rightarrow ((\text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C)\ s' )\ s')\ a \\[5pt]
\quad\equiv\quad (\text{application}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C)\ a )\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C[a/y])\ a
\end{array}
\]

\underline{Right Hand Side}
\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (k\ ())\ s\ [a/s, \text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } C[a/y]/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow ((\text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } C[a/y]) ())\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C[a/y])\ a
\end{array}
\]



\subsubsection*{Get-Then-Set}
A \lstinline{get} followed by a \lstinline{set}, updates the state with a new value whilst preserving the former state value by binding it in the continuation. 


\[
\begin{aligned}
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{get}() \left( x. \operatorname{set} \; a \; (y. C) \right) \\
    &\equiv \\
    &\mathsf{\textbf{with}} \; \mathsf{state} \; \mathsf{\textbf{handle}} \\
    &\quad \operatorname{set} \; a \; (y. C[s/x])
\end{aligned}
\]

\underline{Left Hand Side}
\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun } s \rightarrow (k\ s)\ s\ [()/v, \text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } set(a)(y.C)/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun } s \rightarrow ((\text{fun } x \rightarrow \text{\textbf{with} state \textbf{handle} } set(a)(y.C))\ s)\ s \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun } s \rightarrow (\text{\textbf{with} state \textbf{handle} } set(a)(y.C[s/x]))\ s \\[5pt]
\quad\equiv\quad (11) \\[5pt]
\text{fun } s \rightarrow ((\text{fun } \_ \rightarrow (k\ ())\ s')[a/s', \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C[s/x]/k])\ s' \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun } s \rightarrow (\text{fun } \_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C[s/x]) ())\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun } s \rightarrow (\text{\textbf{with} state \textbf{handle} } C[s/x])\ a
\end{array}
\]


\underline{Right Hand Side}
\[ 
\begin{array}{l}
\quad\equiv\quad (11) \\[5pt]
\text{fun }\_ \rightarrow (k\ ())\ s\ [a/s, \text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C[s/x]/k] \\[5pt]
\quad\equiv\quad (\text{subst}) \\[5pt]
\text{fun }\_ \rightarrow (\text{fun } y \rightarrow \text{\textbf{with} state \textbf{handle} } C[s/x]) ()\ a \\[5pt]
\quad\equiv\quad (8) \\[5pt]
\text{fun }\_ \rightarrow (\text{\textbf{with} state \textbf{handle} } C[s/x]\ a)
\end{array}
\]
LHS $\equiv$ RHS

\section{Environment Variable Proofs} \label{environment-variable-proofs}

\chapter{Participants' information sheet}

If you had human participants, include key information that they were given in
an appendix, and point to it from the ethics declaration.

\chapter{Participants' consent form}

If you had human participants, include information about how consent was
gathered in an appendix, and point to it from the ethics declaration.
This information is often a copy of a consent form.


\end{document}
